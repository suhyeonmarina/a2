{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "COXEhZ9NvfL0"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import numpy as np\n",
        "from collections import Counter\n",
        "import random\n",
        "import re\n",
        "import datasets\n",
        "import tqdm\n",
        "import math\n",
        "from functools import partial\n",
        "import math\n",
        "import argparse\n",
        "import os\n",
        "import collections\n",
        "import json\n",
        "import sentencepiece\n",
        "import shutil\n",
        "import copy\n",
        "import multiprocessing\n",
        "import transformers\n",
        "from dataclasses import dataclass, field\n",
        "from evaluate import load\n",
        "\n",
        "# set \"high\" if you have a GPU with compute capability >= 8.0 else \"highest\"\n",
        "torch.set_float32_matmul_precision(\"high\")\n",
        "if torch.cuda.is_available():\n",
        "    torch.backends.cudnn.benchmark = True"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "shyQgpvWvfL2"
      },
      "source": [
        "# Training config"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7M7nu9jUvfL3"
      },
      "outputs": [],
      "source": [
        "## you can modify some options such as batch_size, depending on your environments\n",
        "\n",
        "training_config = {\n",
        "    \"batch_size\": 16,\n",
        "    \"epochs\": 3,\n",
        "    \"lr\": 1e-4,\n",
        "    \"warmup_steps\": 50,\n",
        "    \"device\": \"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
        "\n",
        "    \"gradient_accumulate_steps\": 1,\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qChns3fWvfL3"
      },
      "source": [
        "# Dataset load"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2nSQbIrLvfL3"
      },
      "outputs": [],
      "source": [
        "dataset = datasets.load_dataset(\"lemon-mint/korean_english_parallel_wiki_augmented_v1\",split=\"train\")\n",
        "dataset = dataset.filter(lambda x: len(x['english']) < 8192 and len(x['english']) > 128 and len(x['korean']) < 8192 and len(x['korean']) > 128)\n",
        "valid_set = dataset.select(range(10000))\n",
        "train_set = dataset.select(range(10000, 110000))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YiHTSNAZvfL3"
      },
      "outputs": [],
      "source": [
        "dataset = datasets.load_dataset(\"lemon-mint/korean_english_parallel_wiki_augmented_v1\",split=\"train\")\n",
        "dataset = dataset.filter(lambda x: len(x['english']) < 8192 and len(x['english']) > 128 and len(x['korean']) < 8192 and len(x['korean']) > 128)\n",
        "valid_set = dataset.select(range(10000))\n",
        "train_set = dataset.select(range(10000, 110000))\n",
        "\n",
        "tokenizer = transformers.AutoTokenizer.from_pretrained(\"Helsinki-NLP/opus-mt-ko-en\")\n",
        "additional_special_tokens = {}\n",
        "if tokenizer.pad_token is None:\n",
        "    additional_special_tokens[\"pad_token\"] = \"<pad>\"\n",
        "if tokenizer.eos_token is None:\n",
        "    additional_special_tokens[\"eos_token\"] = \"</s>\"\n",
        "if tokenizer.bos_token is None:\n",
        "    additional_special_tokens[\"bos_token\"] = \"<s>\"\n",
        "tokenizer.add_special_tokens(additional_special_tokens)\n",
        "\n",
        "def collate_fn(batch):\n",
        "    english_corpus = [item[\"english\"] for item in batch]\n",
        "    korean_corpus = [item[\"korean\"] for item in batch]\n",
        "    english_corpus = tokenizer(english_corpus, padding=True, truncation=True, return_tensors=\"pt\", max_length=512, pad_to_multiple_of=64)\n",
        "    korean_corpus = tokenizer(korean_corpus, padding=True, truncation=True, return_tensors=\"pt\", max_length=512, pad_to_multiple_of=64)\n",
        "    labels = korean_corpus[\"input_ids\"].clone()\n",
        "    labels[korean_corpus['attention_mask'].eq(0)] = -100\n",
        "\n",
        "\n",
        "    return {\n",
        "        \"encoder_input_ids\": english_corpus[\"input_ids\"],\n",
        "        \"encoder_attention_mask\": english_corpus[\"attention_mask\"],\n",
        "        \"decoder_input_ids\": korean_corpus[\"input_ids\"],\n",
        "        \"labels\": korean_corpus[\"input_ids\"],\n",
        "    }\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4JWG6Le4vfL3"
      },
      "source": [
        "### Model implement"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qLzJ1K3DvfL4"
      },
      "outputs": [],
      "source": [
        "@dataclass\n",
        "class ModelConfig(object):\n",
        "    vocab_size: int = field(default=50000)\n",
        "    encoder_hidden_dim: int = field(default=512) # hidden dimention of encoder lstm\n",
        "    decoder_hidden_dim: int = field(default=512) # hidden dimention of decoder lstm\n",
        "    hidden_dim: int = field(default=512) # hidden dimention of other module like attention\n",
        "    embed_dim: int = field(default=512) # embedding dimention\n",
        "    pad_idx: int = field(default=0)\n",
        "    sos_idx: int = field(default=1)\n",
        "    eos_idx: int = field(default=2)\n",
        "    n_layers: int = field(default=1)\n",
        "    dropout: float = field(default=0.1)\n",
        "\n",
        "    attention_type:str = field(default=\"global\")\n",
        "    window_size: int = field(default=10)\n",
        "    sigma_ratio: float = field(default=2.0)\n",
        "\n",
        "    do_input_feeding: bool = field(default=True)\n",
        "\n",
        "class GlobalAttention(nn.Module):\n",
        "    def __init__(self, config: ModelConfig):\n",
        "        super().__init__()\n",
        "        self.config = config\n",
        "\n",
        "        self.query_proj = nn.Linear(config.decoder_hidden_dim, config.hidden_dim, bias=False)\n",
        "        self.key_proj = nn.Linear(config.encoder_hidden_dim * 2, config.hidden_dim, bias=False)\n",
        "        self.value_proj = nn.Linear(config.encoder_hidden_dim * 2, config.hidden_dim, bias=False)\n",
        "        self.output_proj = nn.Linear(config.hidden_dim, config.decoder_hidden_dim, bias=False)\n",
        "\n",
        "        self.dropout = nn.Dropout(config.dropout)\n",
        "        self.scale = np.sqrt(config.hidden_dim)\n",
        "\n",
        "    def forward(self, decoder_hidden_query, encoder_outputs, encoder_attention_mask):\n",
        "        query = self.query_proj(decoder_hidden_query)\n",
        "        key = self.key_proj(encoder_outputs)\n",
        "        value = self.value_proj(encoder_outputs)\n",
        "\n",
        "        # fill here for global attention forward\n",
        "        # shape hint:\n",
        "        # context: (batch, 1, hidden_dim)\n",
        "        ######\n",
        "\n",
        "        ## YOUR CODES\n",
        "\n",
        "        ######\n",
        "        output_context = self.output_proj(context)\n",
        "\n",
        "        return output_context\n",
        "\n",
        "class LocalAttention(GlobalAttention):\n",
        "    def __init__(self, config: ModelConfig):\n",
        "        super().__init__(config)\n",
        "        self.window_size = config.window_size\n",
        "        self.location_proj_up = nn.Linear(config.decoder_hidden_dim, config.hidden_dim, bias=False)\n",
        "        self.location_proj_down = nn.Linear(config.hidden_dim, 1, bias=False)\n",
        "        self.sigma = self.window_size / config.sigma_ratio\n",
        "\n",
        "    def forward(self, decoder_hidden_query, encoder_outputs, encoder_attention_mask):\n",
        "        key, value, attn_mask, gaussian_penalty = self._gather_local_context(decoder_hidden_query, encoder_outputs, encoder_attention_mask)\n",
        "        query = self.query_proj(decoder_hidden_query)\n",
        "        key = self.key_proj(key)\n",
        "        value = self.value_proj(value)\n",
        "\n",
        "        # fill here for local attention forward\n",
        "        # shape hint:\n",
        "        # context: (batch, 1, hidden_dim)\n",
        "        ######\n",
        "\n",
        "        ## YOUR CODES\n",
        "\n",
        "        ######\n",
        "        output_context = self.output_proj(context)\n",
        "\n",
        "        return output_context\n",
        "\n",
        "    def _gather_local_context(self, decoder_hidden_query, encoder_outputs, encoder_attention_mask):\n",
        "        device = encoder_outputs.device\n",
        "        src_len = encoder_attention_mask.sum(dim=-1).unsqueeze(-1)\n",
        "\n",
        "        # fill here for local context window\n",
        "        # shape hint:\n",
        "        # local_key: (batch, window_size * 2 + 1, hidden_dim)\n",
        "        # local_value: (batch, window_size * 2 + 1, hidden_dim)\n",
        "        # local_attn_mask: (batch, window_size * 2 + 1)\n",
        "        # gaussian_penalty: (batch, window_size * 2 + 1)\n",
        "        ######\n",
        "\n",
        "        ## YOUR CODES\n",
        "\n",
        "        ######\n",
        "\n",
        "        return local_key, local_value, local_attn_mask, gaussian_penalty\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self, config: ModelConfig):\n",
        "        super().__init__()\n",
        "        self.config = config\n",
        "\n",
        "        self.encoder = nn.LSTM(\n",
        "            input_size=config.embed_dim,\n",
        "            hidden_size=config.encoder_hidden_dim,\n",
        "            num_layers=config.n_layers,\n",
        "            dropout=config.dropout if config.n_layers > 1 else 0,\n",
        "            bidirectional=True,\n",
        "            batch_first=True\n",
        "        )\n",
        "\n",
        "        self.h_dec_proj = nn.Linear(config.encoder_hidden_dim * 2, config.decoder_hidden_dim)\n",
        "        self.c_dec_proj = nn.Linear(config.encoder_hidden_dim * 2, config.decoder_hidden_dim)\n",
        "\n",
        "    def forward(self, input_embeds, attention_mask):\n",
        "\n",
        "        # Fill here for encoder forward\n",
        "        # shape hint\n",
        "        # input_embeds: (batch, src_seq_len, embed_dim)\n",
        "        # attention_mask: (batch, src_seq_len)\n",
        "        # encoder_output: (batch, src_seq_len, hidden_dim)\n",
        "        # h_enc: (n_layers, batch, decoder_hidden_dim)\n",
        "        # c_enc: (n_layers, batch, decoder_hidden_dim)\n",
        "        # hint for implementation\n",
        "        # 1. use nn.utils.rnn.pack_padded_sequence to packing inputs for rnn series, see https://pytorch.org/docs/stable/generated/torch.nn.utils.rnn.pack_padded_sequence.html\n",
        "        #    failure to properly handle padding will result in a penalty.\n",
        "        # 2. lstm cell state and hidden state will be doubled because of bidirectional lstm.\n",
        "        #    decoder will be unidirectional for causal language modeling.\n",
        "        #    handle the hidden state and cell state to be same as decoder.\n",
        "        ######\n",
        "\n",
        "        ## YOUR CODES\n",
        "\n",
        "        ######\n",
        "\n",
        "        return encoder_output, (h_enc, c_enc)\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self, config: ModelConfig):\n",
        "        super().__init__()\n",
        "        self.config = config\n",
        "\n",
        "        self.decoder = nn.LSTM(\n",
        "            input_size=config.embed_dim + config.hidden_dim if config.do_input_feeding else config.embed_dim,\n",
        "            hidden_size=config.decoder_hidden_dim,\n",
        "            num_layers=config.n_layers,\n",
        "            dropout=config.dropout if config.n_layers > 1 else 0,\n",
        "            batch_first=True\n",
        "        )\n",
        "        match config.attention_type:\n",
        "            case \"local\":\n",
        "                self.attention = LocalAttention(config)\n",
        "            case \"global\":\n",
        "                self.attention = GlobalAttention(config)\n",
        "            case _:\n",
        "                raise ValueError(f\"Unknown attention type: {config.attention_type}\")\n",
        "        self.dropout = nn.Dropout(config.dropout)\n",
        "\n",
        "    def forward(self, input_embeds, encoder_outputs, h_enc, c_enc, attention_mask):\n",
        "        decoder_output, (h_dec, c_dec) = self.decoder(input_embeds, (h_enc, c_enc))\n",
        "        attention_context = self.attention(decoder_output, encoder_outputs, attention_mask)\n",
        "        decoder_output = decoder_output + attention_context\n",
        "\n",
        "        return decoder_output, attention_context, (h_dec, c_dec)\n",
        "\n",
        "class Seq2Seq(nn.Module):\n",
        "    def __init__(self, config: ModelConfig):\n",
        "        super().__init__()\n",
        "        self.config = config\n",
        "\n",
        "        self.embedding = nn.Embedding(config.vocab_size, config.embed_dim, padding_idx=config.pad_idx)\n",
        "\n",
        "        self.encoder = Encoder(config)\n",
        "        self.decoder = Decoder(config)\n",
        "\n",
        "        self.lm_head = nn.Linear(config.hidden_dim, config.vocab_size)\n",
        "        self.dropout = nn.Dropout(config.dropout)\n",
        "\n",
        "    def forward(self, encoder_input_ids, encoder_attention_mask, decoder_input_ids, labels=None, cache=None):\n",
        "        if cache is None:\n",
        "            encoder_input_embeds = self.embedding(encoder_input_ids)\n",
        "            encoder_outputs, (h_enc, c_enc) = self.encoder(encoder_input_embeds, encoder_attention_mask)\n",
        "\n",
        "            current_h_dec, current_c_dec = h_enc, c_enc\n",
        "            prev_attn_context = None\n",
        "        else:\n",
        "            encoder_outputs, current_h_dec, current_c_dec, prev_attn_context = cache\n",
        "\n",
        "        batch_size, tgt_len = decoder_input_ids.shape\n",
        "        decoder_input_embeds = self.embedding(decoder_input_ids)\n",
        "\n",
        "        if prev_attn_context is None:\n",
        "            prev_attn_context = torch.zeros((batch_size, 1, self.config.decoder_hidden_dim)).to(decoder_input_embeds)\n",
        "\n",
        "        outputs = []\n",
        "\n",
        "        for t in range(tgt_len):\n",
        "            # fill here for decoder forward\n",
        "            ######\n",
        "\n",
        "            ## YOUR CODES\n",
        "\n",
        "            ######\n",
        "            outputs.append(decoder_output)\n",
        "\n",
        "\n",
        "        outputs = torch.cat(outputs, dim=1)\n",
        "\n",
        "        lm_logits = self.lm_head(outputs)\n",
        "\n",
        "        loss = None\n",
        "        if labels is not None:\n",
        "            # for cross entropy loss\n",
        "            # loss must be scalar\n",
        "\n",
        "            labels_for_loss = labels[:, 1:].contiguous()\n",
        "            lm_logits_for_loss = lm_logits[:, :-1, :].contiguous()\n",
        "            loss = F.cross_entropy(lm_logits_for_loss.view(-1, self.config.vocab_size), labels_for_loss.view(-1))\n",
        "\n",
        "            return loss\n",
        "        else:\n",
        "            return lm_logits, (encoder_outputs, current_h_dec, current_c_dec, prev_attn_context)\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def generate(\n",
        "        self,\n",
        "        encoder_input_ids: torch.LongTensor,\n",
        "        encoder_attention_mask: torch.LongTensor,\n",
        "        max_new_tokens: int = 256,\n",
        "    ):\n",
        "        batch_size, _ = encoder_input_ids.shape\n",
        "        device = encoder_input_ids.device\n",
        "        eos = self.config.eos_idx\n",
        "\n",
        "        unfinish_flag = torch.ones(batch_size, dtype=torch.long, device=device)\n",
        "        cache = None\n",
        "        decoder_input_ids = torch.full((batch_size, 1), self.config.sos_idx, dtype=torch.long, device=device)\n",
        "\n",
        "        for _ in range(max_new_tokens):\n",
        "            # fill here for causal generation\n",
        "           ######\n",
        "\n",
        "            ## YOUR CODES\n",
        "\n",
        "            ######\n",
        "        return decoder_input_ids\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yK0eARIKvfL4"
      },
      "outputs": [],
      "source": [
        "def train(model, train_dataset, valid_dataset, collate_fn, train_args, prefix):\n",
        "    optimizer = optim.Adam(model.parameters(), lr=train_args[\"lr\"])\n",
        "\n",
        "    train_dataloader = DataLoader(train_dataset, batch_size=train_args['batch_size'], shuffle=True, collate_fn=collate_fn, num_workers=os.cpu_count())\n",
        "    valid_dataloader = DataLoader(valid_dataset, batch_size=train_args['batch_size'], shuffle=False, collate_fn=collate_fn, num_workers=os.cpu_count())\n",
        "\n",
        "    total_steps = len(train_dataloader) * train_args['epochs']\n",
        "\n",
        "    num_training_steps = train_args['epochs'] * (len(train_dataloader) // train_args['gradient_accumulate_steps'])\n",
        "    scheduler = transformers.get_scheduler(\n",
        "        name=\"cosine\",\n",
        "        optimizer=optimizer,\n",
        "        num_warmup_steps=train_args['warmup_steps'],\n",
        "        num_training_steps=num_training_steps\n",
        "    )\n",
        "\n",
        "    best_loss = 987654321\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    output_path = os.path.join(\"output\", prefix)\n",
        "    os.makedirs(output_path, exist_ok=True)\n",
        "    with open(os.path.join(output_path, \"train_args.json\"), \"w\") as f:\n",
        "        json.dump(train_args, f)\n",
        "\n",
        "    pbar = tqdm.tqdm(total=total_steps, desc=\"training\")\n",
        "    for epoch in range(train_args['epochs']):\n",
        "        pbar.set_description(f\"Epoch {epoch+1}/{train_args['epochs']}\")\n",
        "        move_avg_loss = []\n",
        "        model.train()\n",
        "        for i, batch in enumerate(train_dataloader):\n",
        "            batch = {k:v.to(train_args['device']) if isinstance(v,torch.Tensor) else v for k,v in batch.items()}\n",
        "\n",
        "            loss = model(**batch)\n",
        "            loss = loss / train_args['gradient_accumulate_steps']\n",
        "            if loss.size() != torch.Size([]):\n",
        "                loss = loss.mean()\n",
        "            loss.backward()\n",
        "\n",
        "            if (i+1) % train_args['gradient_accumulate_steps'] == 0:\n",
        "                torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "                optimizer.step()\n",
        "                optimizer.zero_grad()\n",
        "                scheduler.step()\n",
        "\n",
        "            move_avg_loss.append(loss.item())\n",
        "            if len(move_avg_loss) > 100: move_avg_loss.pop(0)\n",
        "            pbar.set_postfix_str(f\"loss: {sum(move_avg_loss)/len(move_avg_loss):.04f} lr: {optimizer.param_groups[0]['lr']:.2e}\")\n",
        "            pbar.update(1)\n",
        "\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            eval_loss = 0\n",
        "            for i, batch in enumerate(valid_dataloader):\n",
        "                batch = {k:v.to(train_args['device']) if isinstance(v,torch.Tensor) else v for k,v in batch.items()}\n",
        "                loss_val = model(**batch)\n",
        "                if loss_val.size() != torch.Size([]):\n",
        "                    loss_val = loss_val.mean()\n",
        "                eval_loss += loss_val.item()\n",
        "                pbar.set_postfix_str(f\"val_loss: {eval_loss / (i+1):.04f}\")\n",
        "        eval_loss /= len(valid_dataloader)\n",
        "        pbar.write(f\"Validation Loss: {eval_loss:.04f}\")\n",
        "\n",
        "        if eval_loss < best_loss:\n",
        "            best_loss = eval_loss\n",
        "\n",
        "            torch.save(model.state_dict(), os.path.join(output_path,\"best_model.pth\"))\n",
        "            pbar.write(f\"Model Saved best loss: {best_loss:.04f}\")\n",
        "\n",
        "    pbar.close()\n",
        "\n",
        "def evaluate(model, dataset, tokenizer, collate_fn, train_args):\n",
        "    model.eval()\n",
        "    dataloader = DataLoader(dataset, batch_size=train_args['batch_size'], shuffle=False, collate_fn=collate_fn, num_workers=os.cpu_count())\n",
        "\n",
        "    answers = []\n",
        "    predicts = []\n",
        "    for i, batch in enumerate(tqdm.tqdm(dataloader, desc=\"Evaluating\")):\n",
        "        batch = {k:v.to(train_args['device']) if isinstance(v,torch.Tensor) else v for k,v in batch.items()}\n",
        "        gen_output = model.generate(\n",
        "            encoder_input_ids=batch[\"encoder_input_ids\"],\n",
        "            encoder_attention_mask=batch[\"encoder_attention_mask\"],\n",
        "            max_new_tokens=512\n",
        "        )\n",
        "        pred = tokenizer.batch_decode(gen_output, skip_special_tokens=True)\n",
        "        ans = tokenizer.batch_decode(batch[\"labels\"], skip_special_tokens=True)\n",
        "        answers.extend(ans)\n",
        "        predicts.extend(pred)\n",
        "\n",
        "    bleu = load(\"bleu\")\n",
        "    result = bleu.compute(predictions=predicts, references=answers)\n",
        "    print(f\"BLEU: {result['bleu']:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TBQP3HWCvfL5"
      },
      "outputs": [],
      "source": [
        "config = ModelConfig(\n",
        "    vocab_size=len(tokenizer),\n",
        "    pad_idx=tokenizer.pad_token_id,\n",
        "    sos_idx=tokenizer.bos_token_id,\n",
        "    eos_idx=tokenizer.eos_token_id,\n",
        "    n_layers=2,\n",
        "    dropout=0.1,\n",
        "\n",
        "    attention_type=\"global\",\n",
        "    do_input_feeding=False,\n",
        ")\n",
        "\n",
        "model = Seq2Seq(config).to(training_config[\"device\"])\n",
        "model = model.to(torch.bfloat16)\n",
        "model.compile()\n",
        "print(model)\n",
        "\n",
        "train(\n",
        "    model,\n",
        "    train_set,\n",
        "    valid_set,\n",
        "    collate_fn,\n",
        "    training_config,\n",
        "    prefix=\"seq2seq_global_attention_no_input_feeding\"\n",
        ")\n",
        "\n",
        "model.load_state_dict(torch.load(os.path.join(\"output\", \"seq2seq_global_attention_no_input_feeding\", \"best_model.pth\")))\n",
        "evaluate(\n",
        "    model,\n",
        "    valid_set,\n",
        "    tokenizer,\n",
        "    collate_fn,\n",
        "    training_config\n",
        ")\n",
        "\n",
        "del model\n",
        "torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M8aeveARvfL5"
      },
      "outputs": [],
      "source": [
        "config = ModelConfig(\n",
        "    vocab_size=len(tokenizer),\n",
        "    pad_idx=tokenizer.pad_token_id,\n",
        "    sos_idx=tokenizer.bos_token_id,\n",
        "    eos_idx=tokenizer.eos_token_id,\n",
        "    n_layers=2,\n",
        "    dropout=0.1,\n",
        "\n",
        "    attention_type=\"global\",\n",
        ")\n",
        "\n",
        "model = Seq2Seq(config).to(training_config[\"device\"])\n",
        "model = model.to(torch.bfloat16)\n",
        "model.compile()\n",
        "print(model)\n",
        "\n",
        "train(\n",
        "    model,\n",
        "    train_set,\n",
        "    valid_set,\n",
        "    collate_fn,\n",
        "    training_config,\n",
        "    prefix=\"seq2seq_global_attention\"\n",
        ")\n",
        "\n",
        "model.load_state_dict(torch.load(os.path.join(\"output\", \"seq2seq_global_attention\", \"best_model.pth\")))\n",
        "evaluate(\n",
        "    model,\n",
        "    valid_set,\n",
        "    tokenizer,\n",
        "    collate_fn,\n",
        "    training_config\n",
        ")\n",
        "\n",
        "del model\n",
        "torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KeZQUKl3vfL5"
      },
      "outputs": [],
      "source": [
        "config = ModelConfig(\n",
        "    vocab_size=len(tokenizer),\n",
        "    pad_idx=tokenizer.pad_token_id,\n",
        "    sos_idx=tokenizer.bos_token_id,\n",
        "    eos_idx=tokenizer.eos_token_id,\n",
        "    n_layers=2,\n",
        "    dropout=0.1,\n",
        "\n",
        "    attention_type=\"local\",\n",
        ")\n",
        "\n",
        "model = Seq2Seq(config).to(training_config[\"device\"])\n",
        "model = model.to(torch.bfloat16)\n",
        "model.compile()\n",
        "print(model)\n",
        "\n",
        "train(\n",
        "    model,\n",
        "    train_set,\n",
        "    valid_set,\n",
        "    collate_fn,\n",
        "    training_config,\n",
        "    prefix=\"seq2seq_local_attention\"\n",
        ")\n",
        "\n",
        "model.load_state_dict(torch.load(os.path.join(\"output\", \"seq2seq_local_attention\", \"best_model.pth\")))\n",
        "evaluate(\n",
        "    model,\n",
        "    valid_set,\n",
        "    tokenizer,\n",
        "    collate_fn,\n",
        "    training_config\n",
        ")\n",
        "\n",
        "del model\n",
        "torch.cuda.empty_cache()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "ihatenotebook",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.11"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}